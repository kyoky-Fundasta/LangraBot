# %%

import os
import re


# from graphviz import Digraph
# from graphviz import Graph
# import re
# from dotenv import load_dotenv
# from typing import TypedDict

# from langchain_upstage import UpstageGroundednessCheck
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser
from langchain_google_genai import ChatGoogleGenerativeAI

# from module.vector.pinecone import retrieve_document
from module.web.tavily import search_on_web

from data.const import (
    env_openai,
    env_genai,
    index_name,
    embedding_model,
    env_tavily,
    env_pinecone,
    GraphState,
    gpt_model_name,
)
from module.llm.get_response import advanced_question, normal_question
from langgraph.graph import END, StateGraph
from langgraph.checkpoint.memory import MemorySaver
from IPython.display import Image, display

import pprint
from langgraph.errors import GraphRecursionError
from langchain_core.runnables import RunnableConfig
from module.llm.my_agent import ai_agent
from module.llm.relevancy_test import groundedness_check, is_grounded
from module.llm.editor import rewrite_question


# Question-Answer check
def relevance_check(state: GraphState) -> GraphState:
    print("\n\n$$$$$$check!!!!!!!$$$$$$$\n\n")
    # Double check. result: Yes, No, Not_sure
    prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                "ã‚ãªãŸã¯AIã‚¢ã‚·ã‚¹ãƒˆãŒãŒãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«ã¡ã‚ƒã‚“ã¨ç­”ãˆã‚‹ã“ã¨ãŒå‡ºæ¥ãŸã‹ã©ã†ã‹ã‚’åˆ¤å®šã‚’ã™ã‚‹ã‚¸ãƒ£ãƒƒã‚¸ã§ã™ã€‚è³ªå•ãŒè¤‡æ•°ã‚ã‚‹æ™‚ã«ã¯æœ€å¾Œã®è³ªå•ã¨AIã‚¢ã‚·ã‚¹ãƒˆã®å›ç­”ã§åˆ¤å®šã—ã¦ãã ã•ã„ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æœ€å¾Œã®è³ªå•ã«å¯¾ã—ã¦AIã‚¢ã‚·ã‚¹ãƒˆã®å›ç­”ãŒã—ã£ã‹ã‚Šã¨é–¢é€£æ€§ã‚’ã‚‚ã£ã¦æ­£ã—ãç­”ãˆã‚‰ã‚Œã¦ã„ã‚Œã°(Yes)ã€AIã‚¢ã‚·ã‚¹ãƒˆã®å›ç­”ã«ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã¸ã®ç­”ãˆãŒå…¥ã£ã¦ãªã‹ã£ãŸã‚‰(No)ã€åˆ¤æ–­ã«å›°é›£ãªå ´åˆã¯(Not_sure)ã¨ç­”ãˆã¦ãã ã•ã„ã€‚è³ªå•ãŒè¤‡æ•°ã‚ã‚‹æ™‚ã«ã¯æœ€å¾Œã®è³ªå•ã¨AIã‚¢ã‚·ã‚¹ãƒˆã®å›ç­”ã§åˆ¤å®šã—ã¦ãã ã•ã„",
            ),
            (
                "human",
                "ã¾ãšãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã¨AIã‚¢ã‚·ã‚¹ãƒˆã®å›ç­”ã‚’èª­ã‚“ã§ãã ã•ã„"
                "\n\nãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•:\n ------- \n{question}\n ------- \n"
                "\n\nAIã‚¢ã‚·ã‚¹ãƒˆã®å›ç­”:\n ------- \n{answer}\n ------- \n"
                "\n\nAIã‚¢ã‚·ã‚¹ãƒˆã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«æ­£ã—ãç­”ãˆã‚‹ã“ã¨ãŒå‡ºæ¥ã¾ã—ãŸã‹ã€‚åˆ¤å®šã‚’ã—ã¦ãã ã•ã„"
                "\n\nè³ªå•ãŒè¤‡æ•°ã‚ã‚‹æ™‚ã«ã¯æœ€å¾Œã®è³ªå•ã¨AIã‚¢ã‚·ã‚¹ãƒˆã®å›ç­”ã§åˆ¤å®šã—ã¦ãã ã•ã„ã€‚"
                "\n\nã‚ãªãŸã¯'Yes'ã‹'No'ã‹'Not_sure'ã®ã©ã‚Œã‹ã§ç­”ãˆãªã‘ã‚Œã°ãªã‚Šã¾ã›ã‚“"
                "\n\nèª¬æ˜ã¯è¦ã‚Šã¾ã›ã‚“ã€‚ã‚ãªãŸã®ç­”ãˆã ã‘è¿”ã—ã¦ãã ã•ã„ã€‚",
            ),
        ]
    )

    model = ChatOpenAI(temperature=0, model=gpt_model_name, openai_api_key=env_openai)
    chain = prompt | model | StrOutputParser()
    response = chain.invoke({"question": state["question"], "answer": state["answer"]})
    print("Check response :", response)
    return GraphState(
        question=state["question"],
        context=state["context"],
        answer=state["answer"],
        relevance=response,
        chat_history=state["chat_history"],
        web=state["web"],
    )


# First Question-Answer check
def relevance_check_first(state: GraphState) -> GraphState:
    # Double check. result: Yes, No, Not_sure
    prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                "ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã®æ„å›³ã‚’æŠŠæ¡ã—ã¦ãã ã•ã„ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯æ ªå¼ä¼šç¤¾FundastAã«ã¤ã„ã¦èã„ã¦ã¾ã™ã‹ã€‚æ ªå¼ä¼šç¤¾FundastAã«ã¤ã„ã¦èã„ã¦ã‚‹å ´åˆã«ã¯(Yes)ã€ãã‚Œä»¥å¤–ã®å ´åˆã¯(No)ã¨ç­”ãˆã¦ãã ã•ã„",
            ),
            (
                "human",
                "ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã‚’èª­ã‚“ã§ãã ã•ã„"
                "\n\nãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•:\n ------- \n{question}\n ------- \n"
                "\n\nã‚ã‚‹ä¼šç¤¾ã«ã¤ã„ã¦èã„ã¦ã‚‹ã‘ã©ç¤¾åã‚’è¨€ã‚ãªã‹ã£ãŸå ´åˆã«ã¯FundastAã«ã¤ã„ã¦èã„ã¦èã„ã¦ã‚‹ã¨åˆ¤å®šã—ã¦(Yes)ã¨ç­”ãˆã¦ãã ã•ã„"
                "\n\nå¿…ãš'Yes'ã‹'No'ã§ç­”ãˆã¦ãã ã•ã„"
                "\n\nèª¬æ˜ã¯è¦ã‚Šã¾ã›ã‚“ã€‚ã‚ãªãŸã®ç­”ãˆã ã‘è¿”ã—ã¦ãã ã•ã„ã€‚",
            ),
        ]
    )

    model = ChatOpenAI(temperature=0, model=gpt_model_name, openai_api_key=env_openai)
    chain = prompt | model | StrOutputParser()
    response = chain.invoke({"question": state["question"]})

    return GraphState(
        question=state["question"],
        context=state["context"],
        answer=state["answer"],
        relevance=response,
        chat_history=state["chat_history"],
        web=state["web"],
    )


def ask_with_hint():
    pass


def summarize_final_answer(chat_state, groundedness_check_final):
    final_response = {
        "question": "",
        "answer": "",
        "groundedness": "",
        "reasoning": "",
        "source": "",
    }
    if groundedness_check_final["result"] == "grounded":
        final_response["question"] = chat_state["question"]
        final_response["answer"] = chat_state["answer"]
        final_response["groundedness"] = groundedness_check_final["result"]
        final_response["reasoning"] = groundedness_check_final["reasoning"]
        final_response["source"] = groundedness_check_final["source"]
    else:
        final_response["question"] = chat_state["question"]
        final_response["answer"] = (
            "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€‚è©³ã—ã„æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚æ‹…å½“éƒ¨ç½²ã«ãŠå•ã„åˆã‚ã›ãã ã•ã„ã€‚"
        )
        final_response["groundedness"] = groundedness_check_final["result"]
        final_response["reasoning"] = groundedness_check_final["reasoning"]
        final_response["source"] = ""
    return final_response


def invoke_chain(app, chat_state, confic):
    result_1 = app.invoke(
        chat_state,
        confic={"selected_model": confic["selected_model"]},
        node="rewrite_question",
    )
    result_2 = app.invoke(
        chat_state,
        confic={
            "selected_model": confic["selected_model"],
            "rewrited_question": result_1,
        },
        node="advanced_question",
    )
    result_3 = app.invoke(
        chat_state,
        confic={"selected_model": confic["selected_model"]},
        node="groundedness_check",
    )
    return result_3


# if __name__ == "__main__":
def chat(user_question, chat_history, model_name, who):

    # Generate answer with LLM
    chat_state = GraphState(
        question=user_question,
        context="",
        web="",
        answer="",
        relevance="",
        chat_history=chat_history,
        hint="",
    )

    if who == "Guest":
        final_answer = normal_question(model_name, chat_state)
        return final_answer + "\nğŸ‘¦ Guest mode"

    elif who == "FundastA_ç¤¾å“¡":
        chat_state_agent = ai_agent(model_name, chat_state)
        groundedness_check_agent = groundedness_check(chat_state_agent, model_name)

        if groundedness_check_agent["result"] == "grounded":
            _ = summarize_final_answer(chat_state, groundedness_check_agent)
            print("\n\n----------------Answering routine 1---------------------\n\n", _)
            return _

        else:
            workflow = StateGraph(GraphState)

            workflow.add_node("rewrite_question", rewrite_question)
            workflow.add_node("advanced_question", advanced_question)
            workflow.add_node("groundedness_check", groundedness_check)

            # Connect nodes to each other
            workflow.add_edge("rewrite_question", "advanced_question")
            workflow.add_edge("advanced_question", "groundedness_check")

            # If statement
            workflow.add_conditional_edges(
                "groundedness_check",
                is_grounded,
                {
                    "grounded": END,  # Finish
                    "not grounded": "rewrite_question",  # Rewrite question
                    "not sure": "rewrite_question",  # Rewrite question
                },
            )

            workflow.set_entry_point("rewrite_question")
            memory = MemorySaver()
            app = workflow.compile(checkpointer=memory)

            #   Draw a diagram describing reasoning flow
            try:
                graph = app.get_graph(xray=True)
                # Using draw_mermaid_png to render the graph
                png_bytes = graph.draw_mermaid_png()
                if png_bytes:
                    # Display the graph
                    display(Image(png_bytes))
                else:
                    print("No PNG bytes generated")
            except Exception as e:
                print(f"An error occurred: {e}")

            config = RunnableConfig(
                recursion_limit=6, configurable={"thread_id": "CORRECTIVE-SEARCH-RAG"}
            )

            last_output = None

            try:
                for output in invoke_chain.stream(
                    app, chat_state, {"selected_model": model_name}
                ):
                    last_output = output
                    for key, value in output.items():
                        pprint.pprint(f"\nOutput from node '{key}':")
                        pprint.pprint("---")
                        pprint.pprint(value, indent=2, width=80, depth=None)
            except GraphRecursionError as e:
                print(f"Recursion limit reached: {e}")

            _ = summarize_final_answer(chat_state, last_output)
            print("\n\n----------------Answering routine 2---------------------\n\n", _)

            return _


# Example usage
if __name__ == "__main__":
    chat_history = []  # Initialize chat history
    question_1 = "åå¤å±‹å¸‚ã®å±±æœ¬å¹¸å¸ã•ã‚“ãŒCEOã‚’ã‚„ã£ã¦ã„ã‚‹SESä¼šç¤¾ã«è‚²å…ä¼‘æš‡ã¯ã‚ã‚Šã¾ã™ã‹"
    answer_1 = chat(question_1, chat_history, "Gemini_1.5_Flash", "FundastA_ç¤¾å“¡")


# %%
